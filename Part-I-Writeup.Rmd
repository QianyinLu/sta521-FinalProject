---
title: "STA 521 - Final Project Part I"
date: "December 7th, 2019"
author: "FP-Team 01: Qianyin Lu, George Lindner, Chenxi Wu, Yi Mi"
output: pdf_document
---

```{r packages, echo = FALSE, warning=FALSE,message=FALSE}
library(tidyverse)
library(GGally)
library(car)
library(dplyr)
library(stringr)
library(ggpubr)
library(knitr)
library(kableExtra)
```

## 1. Introduction: Summary of problem and objectives

Our team of esteemed statisticians was recently hired by a prestigious art historian for a consulting project. We were asked to help build a predictive model in exchange for an A on our STA 521 Final Exam. After much discussion, our team accepted the historian's offer. 

We were given the task of predicting paintings' selling prices at auctions in 18th century Paris. To accomplish this, we used a dataset containing information about each painting's buyer, seller, painter, and characteristics of the painting. These variables were all possible predictor variables in modeling the response variable, the selling price of a painting. 

There were two primary objectives in our analysis:

1) To determine which variables (or interactions) drove the price of a painting
2) To determine which paintings were overpriced or and which were underpriced. 

After arriving at a final model, we are able to answer these primary questions. Any variables that appear in the model will be important in driving painting prices, and observing residuals will enable us to determine if a painting was over or underpriced. 

We had 1,500 observations to train the model on, along with 750 observations held out as a testing set. There were a total of 59 variables in the dataset, both categorical and continuous.

## 2. Exploratory Data Analysis:
```{r read-data, echo=FALSE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```

```{r To Do #1, echo = FALSE}

# merge the two dataset first
paint <- rbind(paintings_train, paintings_test)

# Fix position > 1
paint$position[paint$position > 1] <- paint$position[paint$position > 1]/100

# regroup some categorical variables
paint <- paint %>%
  mutate(winningbiddertype = as.factor(dplyr::recode(na_if(winningbiddertype,""),
                                              .missing = 'U')),
         endbuyer = as.factor(dplyr::recode(na_if(endbuyer,""),
                                     .missing = 'U')),
         Interm = as.factor(dplyr::recode(Interm,
                                   .missing = 0,
                                   `0` = 0,
                                   `1` = 1)),
         authorstyle = dplyr::recode(authorstyle,
                                      "n/a" = 0,
                                      .default = 1),
         type_intermed = as.factor(dplyr::recode(na_if(type_intermed,''),
                                          .missing='Unknown')),
         Shape = as.factor(dplyr::recode(na_if(Shape,""),
                                  .missing = 'other',
                                  'squ_rect' = 'squ_rect',
                                  .default = 'other')),
         materialCat = as.factor(dplyr::recode(na_if(materialCat,""),
                                        .missing = 'other'))) %>% 

  # change variables into appropriate format
  mutate_at(vars(dealer, origin_author, origin_cat, school_pntg, 
                 diff_origin, artistliving, authorstandard, authorstyle, 
                 winningbiddertype, endbuyer, Interm, type_intermed, 
                 material, mat, materialCat, Shape, engraved, original, 
                 prevcoll, othartist, paired, figures, finished, lrgfont, 
                 relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, 
                 arch, mytho, peasant, othgenre, singlefig, portrait, 
                 still_life, discauth, history, allegory, pastorale,other), as.factor) %>% 
  
  mutate(Surface = dplyr::recode(Surface,
                                     `0` = median(Surface,na.rm=T),
                                 .missing = median(Surface, na.rm=T))) %>%
  mutate(origin_cat = dplyr::recode(origin_cat,
                                    "S" = "O")) %>%
  mutate(nfigures = as.factor(dplyr::recode(nfigures,
                                            `0` = 0,
                                            .default = 1)))

# delete unnecessary variables

paint_ <- paint %>% 
  dplyr::select(-c("sale", "lot", "diff_origin", "count","subject", "author",
                   "authorstandard","winningbidder","type_intermed","material","mat",
                   "Diam_in","Surface_Rnd","Surface_Rect","Width_in","Height_in","other",
                   "pastorale","history","lands_ment","original","allegory","still_life",
                   "portrait", "school_pntg", "origin_author", 
                   "winningbiddertype", "figures", "singlefig", "price"))

# Split back into train and test
paint_train <- paint_ %>% filter(!is.na(logprice))
paint_test <- paint_ %>% filter(is.na(logprice))
```

### Initial Data Cleaning

We began our data cleaning process by reading the codebook for a better understanding of what each variable in the data represented. Several predictors in the dataset were redundant and therefore removed to avoid high correlation among the predictors. Examples of this include the variable *sale*, which is a combination of *dealer* and *year*. Additionally, there were other predictors that we deemed would not be useful for prediction, such as *count* which was 1 for every observation, or *subject* which was a short description of the content in the painting. We simplified the data by eliminating unnecessary predictors.

### Categorical Variables

We recoded each categorical variable to be a factor. We created a visualization of the binary categorical variables to observe the balance between classes below.

### Plot 1

```{r, echo = FALSE, warning=FALSE, message=FALSE}
paint_eda3 <- paint %>% select(-c(origin_author, authorstandard, school_pntg, 
                                  type_intermed, material, mat, sale, lot, position,
                                  year, logprice, price, count, subject, author, 
                                  winningbidder,Height_in, Width_in,
                                  Surface_Rect, Diam_in, Surface_Rnd,Surface,
                                  dealer, origin_cat, diff_origin,
                                  winningbiddertype, endbuyer, 
                                  Shape, materialCat))


eda3 <- paint_eda3 %>% 
  gather(key = "numeric", value  ="value")

g <- ggplot(eda3, aes(numeric)) + scale_fill_brewer(palette = "Spectral")

g <- g + geom_histogram(aes(fill=value),
                   stat = 'count') +
  labs(title="Class Imbalance in Categorical Variables",
       x = 'Variable',
       y = 'Count') +
  scale_fill_discrete(name = "Level") + 
  theme_classic() +
  coord_flip() 
  
g + geom_hline(mapping = aes(yintercept = 100),  linetype = 'dashed')
```
Imbalanced classes can lead to poor $\beta$ estimates if the underrepresented class does not have enough data. This was our motivation to remove any variable that had less than an arbitrary 100 observations in a class, which is denoted by the dotted black line in our visualization above. 

To identify important categorical variables, we created a boxplot for each variable that compared the distribution of *logprice* over every level of the factor. The results are shown below. 

### Plot 2

```{r eda, echo = FALSE, warning=FALSE,message=FALSE}

# create dataframe for eda plots
eda_cat <- cbind(logprice = paint_train$logprice, 
                 paint_train[, map_chr(paint_train, class) == "factor"])

eda_con <- paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>% 
  dplyr::select(c(3, 4, 1 ,2, 3))

eda_cat <- eda_cat %>% 
  gather(key = "categorical", value = "value", -logprice)
eda_con <- eda_con %>% 
  gather(key = "numeric", value  ="value", -logprice)
# split eda_cat since there are too many variables
eda_cat1 <- eda_cat[c(1:19500), ]
eda_cat2 <- eda_cat[c(19501:37500), ]

ggplot(eda_cat1, aes(x = value, y = logprice, colour = categorical)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~categorical, scales = "free") +
  labs(x = "", title = "Boxplots of Log Price for Categorical Variables") +
  theme_bw()

ggplot(eda_cat2, aes(x = value, y = logprice, colour = categorical)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~categorical, scales = "free") +
  labs(x = "", title = "Boxplots of Log Price for Categorical Variables (continued)") +
  theme_bw()

```

The boxplots above help us identify which variables could be important in predicting a painting's price. They also help us in our variable selection process by displaying variables that have similar prices in all of their categories. After inspecting the boxplots, we determined that *mytho*, *landsALL*, *relig*, and *othartist* were not useful for prediction. Variables that may be important include, but are not limited to, *lrgfont*, *Interm*, *authorstyle*, and *prevcoll*. 

### Quantitative Variables

There are also quantitative variables in our data that could be used for prediction. Like the categorical variables, many of these predictors were redundant. For example, we were given the surface area of a painting. Additionally, we were given a variable for surface area if the painting was round and a surface area variable if the painting was rectangular. We also were given the height, the width, and the diameter of the painting. We determined that all this information could be condensed to a single variable, *Surface*. 

There was missing data in *Surface* that we had to address. Surface area intuitively seems like it could drive the price of a painting, so we had to develop a strategy for handling the missing observations. With the help of the plot below, we determined that imputing the median surface area size of the dataset would be a good estimation for missing values. Since the distribution of *Surface* is skewed, we wanted an imputation strategy that would be robust to outliers. Thus, we opted for the median over the mean.  

### Plot 3

```{r more plots, echo = FALSE, warning=FALSE, message=FALSE, eval = FALSE}

ggpairs(paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>% 
  dplyr::select(c(3,1:4)), lower = list(continuous = wrap("points", alpha = 0.5, size = 0.5))) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Pairwise Comparisons of Quantitative Variables")

```

```{r, echo = FALSE, warning=FALSE,message=FALSE}
qnt1 <- ggplot(eda_con, aes(x = value, y = logprice)) +
  geom_point(size = 0.5, alpha = 0.5) +
  stat_smooth(method = "lm") +
  facet_wrap(~numeric, scales = "free") +
  labs(x = "", y = "Log Price", title = "Log Price vs Quantitative Predictors (Pre Surface Transformation)") +
  theme_bw()

paint.train_plt_trans <- paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>%
  filter(Surface!=0) %>%
  mutate('log(Surface)' = log(Surface)) %>%
  select(-Surface) %>%
  tidyr::gather(key = "variables", value = "values", -logprice)

qnt2 <- ggplot(paint.train_plt_trans, aes(x = values, y = logprice)) +
geom_point(alpha = 0.5, size = 0.7) +
stat_smooth(method='lm') +
facet_wrap(~variables, scales = "free_x") +
labs(x = "", y = "Log Price",
title = "Log Price vs Quantitative Predictors (Post Surface Transformation)") +
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggarrange(qnt1, qnt2,
          ncol = 1, nrow = 2)
```

We created scatterplots to observe the relationship between our three quantitative predictor variables and the log price of a painting. The distribution of *Surface* was skewed right and a log transformation was necessary. We plot the relationship of logprice and the transformed Surface column in the lower graph.

After considering our EDA plots, we determined the 10 variables that we thought would be most useful in predicting *logprice*. 

```{r kable important variables, echo = FALSE, message=FALSE, warning=FALSE}
ten_var <- data.frame(Rank = seq(1,10,1), Variable = c('log(Surface)','lrgfont','Interm','authorstyle','prevcoll','origin_cat','engraved','finished','discauth','dealer'))

kable(ten_var, caption = "10 Most Important Predictor Variables, from EDA")

```

With the data cleaned and important variables identified, we move to the next step of the process: modeling the data.


## 3. Development and Assessment of Initial Model:

### Model Development

We considered three additional variables in addition to the ten most important variables identified through EDA. These variables were *lands_sc*, *endbuyer*, and *year*. We chose to include these three variables because the EDA suggested that they might add some predictive power to our model. Approximately 62.2% of the variation in *logprice* can be explained by the predictors in our initial model, according to the summary output below. Next, we used step-wise variable seleciton with AIC as our criteria to ensure that each variable reduced RSS enough to justify including the variable in the model. Step-wise AIC selection returned our full model, indicating that we did a good job selecting predictor variables through the EDA. 

Next, we considered interaction terms for our predictor variables. We again used intuition as our method for introducing interactions in the model. Interactions that we considered were *authorstyle* with *log(Surface)*, *Interm* with *log(Surface)*, and *discauth* with *log(Surface)*. We chose the first term because both the style of a painting and the size of the painting could be important in determining the price. Various styles of paintings might increase in value at different rates as the size of the painting changes. The second interaction term considers the dealer engaging with the authenticity of the painting and the size of the painting. Authentic paintings could increase in value as the size increases at a different rate than non-authentic paintings. We also thought that whether an intermediary was involved could be an important interaction with the size of the painting. 


```{r Model Development, echo = FALSE, warning=FALSE,message=FALSE}

model.1 = lm(logprice ~ log(Surface) + lrgfont + Interm + authorstyle + 
    prevcoll + origin_cat + engraved + finished + discauth + 
    dealer + lands_sc + endbuyer + year, data = paint_train)

model.1.aic = step(model.1, k = 2, trace = F)
```

We fit our initial model on these predictors and interaction terms. Included below are our model plots and the summary of the model. 

### Model Plots

```{r, echo = FALSE, warning = FALSE, message=FALSE}

final_model <- lm(formula = logprice ~ log(Surface) + lrgfont + Interm + authorstyle + 
    prevcoll + origin_cat + engraved + finished + discauth + 
    dealer + lands_sc + endbuyer + year + authorstyle:log(Surface) + Interm:log(Surface) +
      log(Surface):discauth, data = paint_train)

par(mfrow = c(2,2))
plot(final_model)
```

### Model Summary

```{r summary of model, echo = FALSE}
summary(final_model)
```

From the residual vs. fitted plot, we see that our residuals are randomly distributed with mean 0. There is no heteroskedacity satisfying the constant variance assumption of linear regression. Our QQ plot appears approximately normal, as well. The residuals vs fitted plot shows that there are no high leverage points, influential points, or outliers. We can see from the summary output that approximately 62.8% of the variation in *logprice* can be explained by our model. 

```{r, eval = FALSE, echo = FALSE}
options(scipen=999)

tb_final <- broom::tidy(final_model, conf.level=0.95,
                   conf.int = TRUE, exponentiate=F)

kable(tb_final, digits = 3,
      caption="Coefficient Summary for Final Model") %>%
  kable_styling(latex_options = "hold_position")
```

```{r predict-model1, echo=FALSE}
predictions = as.data.frame(
  exp(predict(final_model, newdata=paint_test, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```


## 4. Summary and Conclusions:
Looking at the coefficient summary table, we can get the following conclusion. 
 
 - If the previous owner is mentioned, the price will increase by 126.1% with a 95% confidence interval [-70.9%, 199.1%].
 
 - If the authors name is introduced, the price will decrease by 54.6% with a 95% confidence interval [-56.8%, 86.8%].

 - If an intermediary is involved in the transaction, the price will increase by 8.1% with a 95% confidence interval [-57.2%, 173.1%].
 
 - If the dealer devotes an additional paragraph, the price will increase by 130.6% with a 95% confidence interval [81.8%, 192.2%].
 
 - Holding all other variables constant, we expect 10% increase in *Surface* will increase the price by 3.4% ($1.1^{\hat{\beta}_1} - 1$). Specifically, if the authors name is introduced, a 10% increase in *Surface* will result in 3% increase of the price; If there is an intermediary involved, a 10% increase will result in 4.47% increase of the price; If the dealer engages with the authenticity of the painting, a 10% increase will result in 2.5% increase of the price. 
 
 - Holding all other variables constant, if the dealer devotes an additional paragraph, the price will increase by 130.6% ($e^{\hat{\beta_2}}$)

 - If there is an intermediary involved in the transaction, we expect the price goes up by 8.1%. 
 
 - If the previous owner is mentioned, the price is expected to increase by 126.1%.
 - On average, one unit increase in year will lead to 13.4% increase in price.

 - Compared to a buyer endbuyer, a collecter endbuyer will lead to 25.6decrease in price; A dealer endbuyer will lead to 39.5% decrease in price; An expert endbuyer will lead to 58.9% decrease in price; An unknown endbuyer will lead to 77% decrease in price.

 - If the painting is described as a plain landscape, the price will decrease by 34.5%.

 - Compared to dealer J, dealer L will lead to 266.2% increase in price; Dealer P will lead to 37.1% increase in price; Dealer R will lead to 499.8% increase in price.

 - If the dealer engages with the authenticity of the painting, the price will increase by 186.7%.

 - If the the painting is finished, the price will increase by 122.8%.

 - If the dealer mentions engravings done after the painting, the price will increase by 104.2%.

 - Compared to Spanish, French origin will lead to 57% decrease in price; Italian will lead to 51.7% decrease in price; Other origin will lead to 57.1% decrease in price.
  
The most important variables are `Surface`, `lrgfont`, `prevcoll`, `origin_cat`, `engraved`, `finished` ,`dealer`, `year`.

In our model, the confidence intervals for Surface, Irgfont1, prevcoll1, engraved1, finished1, dealerL, dealerP, dealerR, year strictly excludes 0 and thus, includes all positive values. However, notice that when surface interact with authorstyle, interm and discauth, confidence intervals that do not guarantee to include all postive values. Thus, we should condider this scenario as well. However, considering that the interaction terms themselves are not strictly positive, we can still consider surface as a preferrable feature. This means larger surface size, later years, dealer being able to devote an additional paragraph or mentioning engravings done after the painting, previous owner being mentioned, painting being finished, and the dealer of the painting is L,P or R can all contribute positively to the auction price. Paintings with such combined features tend to have higher auction price according to our model.

### Limitations 

There are a few limitations of our model. A major limitation is not having sufficient coverage for test predictions. We only have a 65% coverage rate for our 95% prediction interval. Another limitation is the year variable. While this linear regression might work for this short time frame, this analysis suggests that older paintings are worth less than newer paintings. However, this doesn't capture the 'antique market' for classic pieces of art, which are extremely expensive. This limitation is clear when observing the intercept. The intercept assumes the year is 0, which is not logical for this analysis.  

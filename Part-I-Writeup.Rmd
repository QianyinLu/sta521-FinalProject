---
title: "STA 521 - Final Project Part I"
date: "December 7th, 2019"
author: "FP-Team 01: Qianyin Lu, George Lindner, Chenxi Wu, Yi Mi"
output: pdf_document
---

```{r packages, echo = FALSE, warning=FALSE,message=FALSE}
library(tidyverse)
library(GGally)
library(car)
library(dplyr)
library(stringr)
library(ggpubr)
library(knitr)
library(kableExtra)
```

## 1. Introduction: Summary of problem and objectives

Our team of esteemed statisticians was recently hired by a prestigious art historian for a consulting project. We were asked to help build a predictive model in exchange for an A on our STA 521 Final Exam. After much discussion, our team accepted the historian's offer. 

We were given the task of predicting paintings' selling prices at auctions in 18th century Paris. To accomplish this, we used a dataset containing information about each painting's buyer, seller, painter, and characteristics of the painting. These variables were all possible predictor variables in modeling the response variable, the selling price of a painting. 

There were two primary objectives in our analysis:

1) To determine which variables (or interactions) drove the price of a painting
2) To determine which paintings were overpriced or and which were underpriced. 

After arriving at a final model, we are able to answer these primary questions. Any variables that appear in the model will be important in driving painting prices, and observing residuals will enable us to determine if a painting was over or underpriced. 

We had 1,500 observations to train the model on, along with 750 observations held out as a testing set. There were a total of 59 variables in the dataset, both categorical and continuous.

## 2. Exploratory Data Analysis:
```{r read-data, echo=FALSE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```

```{r To Do #1, echo = FALSE}

# merge the two dataset first
paint <- rbind(paintings_train, paintings_test)

# Fix position > 1
paint$position[paint$position > 1] <- paint$position[paint$position > 1]/100

# regroup some categorical variables
paint <- paint %>%
  mutate(winningbiddertype = as.factor(dplyr::recode(na_if(winningbiddertype,""),
                                              .missing = 'U')),
         endbuyer = as.factor(dplyr::recode(na_if(endbuyer,""),
                                     .missing = 'U')),
         Interm = as.factor(dplyr::recode(Interm,
                                   .missing = 0,
                                   `0` = 0,
                                   `1` = 1)),
         authorstyle = dplyr::recode(authorstyle,
                                      "n/a" = 0,
                                      .default = 1),
         type_intermed = as.factor(dplyr::recode(na_if(type_intermed,''),
                                          .missing='Unknown')),
         Shape = as.factor(dplyr::recode(na_if(Shape,""),
                                  .missing = 'other',
                                  'squ_rect' = 'squ_rect',
                                  .default = 'other')),
         materialCat = as.factor(dplyr::recode(na_if(materialCat,""),
                                        .missing = 'other'))) %>% 

  # change variables into appropriate format
  mutate_at(vars(dealer, origin_author, origin_cat, school_pntg, 
                 diff_origin, artistliving, authorstandard, authorstyle, 
                 winningbiddertype, endbuyer, Interm, type_intermed, 
                 material, mat, materialCat, Shape, engraved, original, 
                 prevcoll, othartist, paired, figures, finished, lrgfont, 
                 relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, 
                 arch, mytho, peasant, othgenre, singlefig, portrait, 
                 still_life, discauth, history, allegory, pastorale,other), as.factor) %>% 
  
  mutate(Surface = dplyr::recode(Surface,
                                     `0` = median(Surface,na.rm=T),
                                 .missing = median(Surface, na.rm=T))) %>%
  mutate(origin_cat = dplyr::recode(origin_cat,
                                    "S" = "O")) %>%
  mutate(nfigures = as.factor(dplyr::recode(nfigures,
                                            `0` = 0,
                                            .default = 1)))

# delete unnecessary variables

paint_ <- paint %>% 
  dplyr::select(-c("sale", "lot", "diff_origin", "count","subject", "author",
                   "authorstandard","winningbidder","type_intermed","material","mat",
                   "Diam_in","Surface_Rnd","Surface_Rect","Width_in","Height_in","other",
                   "pastorale","history","lands_ment","original","allegory","still_life",
                   "portrait", "school_pntg", "origin_author", 
                   "winningbiddertype", "figures", "singlefig", "price"))

# Split back into train and test
paint_train <- paint_ %>% filter(!is.na(logprice))
paint_test <- paint_ %>% filter(is.na(logprice))
```

### Initial Data Cleaning

We began our data cleaning process by reading the codebook for a better understanding of what each variable in the data represented. Several predictors in the dataset were redundant and therefore removed to avoid high correlation among the predictors. Examples of this include the variable *sale*, which is a combination of *dealer* and *year*. Additionally, there were other predictors that we deemed would not be useful for prediction, such as *count* which was 1 for every observation, or *subject* which was a short description of the content in the painting. We simplified the data by eliminating unnecessary predictors.

### Categorical Variables

We recoded each categorical variable to be a factor. We created a visualization of the binary categorical variables to observe the balance between classes below.

### Plot 1

```{r, echo = FALSE, warning=FALSE, message=FALSE}
paint_eda3 <- paint %>% select(-c(origin_author, authorstandard, school_pntg, 
                                  type_intermed, material, mat, sale, lot, position,
                                  year, logprice, price, count, subject, author, 
                                  winningbidder,Height_in, Width_in,
                                  Surface_Rect, Diam_in, Surface_Rnd,Surface,
                                  dealer, origin_cat, diff_origin,
                                  winningbiddertype, endbuyer, 
                                  Shape, materialCat))


eda3 <- paint_eda3 %>% 
  gather(key = "numeric", value  ="value")

g <- ggplot(eda3, aes(numeric)) + scale_fill_brewer(palette = "Spectral")

g <- g + geom_histogram(aes(fill=value),
                   stat = 'count') +
  labs(title="Class Imbalance in Categorical Variables",
       x = 'Variable',
       y = 'Count') +
  scale_fill_discrete(name = "Level") + 
  theme_classic() +
  coord_flip() 
  
g + geom_hline(mapping = aes(yintercept = 100),  linetype = 'dashed')
```
Imbalanced classes can lead to poor $\beta$ estimates if the underrepresented class does not have enough data. This was our motivation to remove any variable that had less than an arbitrary 100 observations in a class, which is denoted by the dotted black line in our visualization above. 

To identify important categorical variables, we created a boxplot for each variable that compared the distribution of *logprice* over every level of the factor. The results are shown below. 

### Plot 2

```{r eda, echo = FALSE, warning=FALSE,message=FALSE}

# create dataframe for eda plots
eda_cat <- cbind(logprice = paint_train$logprice, 
                 paint_train[, map_chr(paint_train, class) == "factor"])

eda_con <- paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>% 
  dplyr::select(c(3, 4, 1 ,2, 3))

eda_cat <- eda_cat %>% 
  gather(key = "categorical", value = "value", -logprice)
eda_con <- eda_con %>% 
  gather(key = "numeric", value  ="value", -logprice)
# split eda_cat since there are too many variables
eda_cat1 <- eda_cat[c(1:19500), ]
eda_cat2 <- eda_cat[c(19501:37500), ]

ggplot(eda_cat1, aes(x = value, y = logprice, colour = categorical)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~categorical, scales = "free") +
  labs(x = "", title = "Boxplots of Log Price for Categorical Variables") +
  theme_bw()

ggplot(eda_cat2, aes(x = value, y = logprice, colour = categorical)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~categorical, scales = "free") +
  labs(x = "", title = "Boxplots of Log Price for Categorical Variables (continued)") +
  theme_bw()

```

The boxplots above help us identify which variables could be important in predicting a painting's price. They also help us in our variable selection process by displaying variables that have similar prices in all of their categories. After inspecting the boxplots, we determined that *mytho*, *landsALL*, *relig*, and *othartist* were not useful for prediction. Variables that may be important include, but are not limited to, *lrgfont*, *Interm*, *authorstyle*, and *prevcoll*. 

### Quantitative Variables

There are also quantitative variables in our data that could be used for prediction. Like the categorical variables, many of these predictors were redundant. For example, we were given the surface area of a painting. Additionally, we were given a variable for surface area if the painting was round and a surface area variable if the painting was rectangular. We also were given the height, the width, and the diameter of the painting. We determined that all this information could be condensed to a single variable, *Surface*. 

There was missing data in *Surface* that we had to address. Surface area intuitively seems like it could drive the price of a painting, so we had to develop a strategy for handling the missing observations. With the help of the plot below, we determined that imputing the median surface area size of the dataset would be a good estimation for missing values. Since the distribution of *Surface* is skewed, we wanted an imputation strategy that would be robust to outliers. Thus, we opted for the median over the mean.  

### Plot 3

```{r more plots, echo = FALSE, warning=FALSE, message=FALSE, eval = FALSE}

ggpairs(paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>% 
  dplyr::select(c(3,1:4)), lower = list(continuous = wrap("points", alpha = 0.5, size = 0.5))) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Pairwise Comparisons of Quantitative Variables")

```

```{r, echo = FALSE, warning=FALSE,message=FALSE}
qnt1 <- ggplot(eda_con, aes(x = value, y = logprice)) +
  geom_point(size = 0.5, alpha = 0.5) +
  stat_smooth(method = "lm") +
  facet_wrap(~numeric, scales = "free") +
  labs(x = "", y = "Log Price", title = "Log Price vs Quantitative Predictors (Pre Surface Transformation)") +
  theme_bw()

paint.train_plt_trans <- paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>%
  filter(Surface!=0) %>%
  mutate('log(Surface)' = log(Surface)) %>%
  select(-Surface) %>%
  tidyr::gather(key = "variables", value = "values", -logprice)

qnt2 <- ggplot(paint.train_plt_trans, aes(x = values, y = logprice)) +
geom_point(alpha = 0.5, size = 0.7) +
stat_smooth(method='lm') +
facet_wrap(~variables, scales = "free_x") +
labs(x = "", y = "Log Price",
title = "Log Price vs Quantitative Predictors (Post Surface Transformation)") +
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggarrange(qnt1, qnt2,
          ncol = 1, nrow = 2)
```

We created scatterplots to observe the relationship between our three quantitative predictor variables and the log price of a painting. The distribution of *Surface* was skewed right and a log transformation was necessary. We plot the relationship of logprice and the transformed Surface column in the lower graph.

After considering our EDA plots, we determined the 10 variables that we thought would be most useful in predicting *logprice*. 

```{r kable important variables, echo = FALSE, message=FALSE, warning=FALSE}
ten_var <- data.frame(Rank = seq(1,10,1), Variable = c('log(Surface)','lrgfont','Interm','authorstyle','prevcoll','origin_cat','engraved','finished','discauth','dealer'))

kable(ten_var, caption = "10 Most Important Predictor Variables, from EDA")

```

With the data cleaned and important variables identified, we move to the next step of the process: modeling the data.


### 3. Development and Assessment of Initial Model:

```{r Cassies Work}



george_model.0 <- lm(logprice ~ log(Surface) + lrgfont + Interm + authorstyle + prevcoll + origin_cat + engraved + finished + discauth + dealer + lands_sc + lands_figs + othgenre +
                endbuyer + nfigures + Shape + year, data = paint_train)
summary(model.0)

model.0.bic = step(model.0, k=log(nrow(paint_train)), trace = F)

summary(model.0.bic)


model.1 = lm(logprice ~ log(Surface) + lrgfont + Interm + authorstyle + 
    prevcoll + origin_cat + engraved + finished + discauth + 
    dealer + lands_sc + endbuyer + year, data = paint_train)

model.1.aic = step(model.1, k = 2, trace = F)

summary(model.1.aic)

```


We first remove some variables because they have very similar levels and thus may not have significant influence on the response.

We use BIC to select the important variables. And we use the BIC result to build a new model and construct all interaction terms and then we use AIC to determine the final model.(may not appear in the write-up for bic part)


We deleted some interactions for the follow reasons. We deleted the interactions `lands_sc:discauth`, `paired:lands_sc` and `artistliving:lands_sc`. First of all, for the interactions related to `lands_sc`, we think the `lands_sc`--if described as a plain landscape, is a variable describing a single dimension of painting content and we don't think it will interact with the variables describing the dealers' behaviour, the pairing of a painting, or the living info of the artists. Though the Adjusted R-squared decreased a little, from 0.66 to 0.659, we think it worth to make the model more reasonable.

```{r}

g_interact <- lm(formula = logprice ~ log(Surface) + lrgfont + Interm + authorstyle + 
    prevcoll + origin_cat + engraved + finished + discauth + 
    dealer + lands_sc + endbuyer + year +authorstyle:log(Surface) + Interm:log(Surface) +
      Interm:prevcoll +
      Interm:lrgfont +
      log(Surface):discauth +
      paired:lrgfont, data = paint_train)

plot(g_interact)

summary(g_interact)
```

### Model Assessment

```{r res plot, fig.asp=1, eval = FALSE}
# plot
par(mfrow = c(2, 2))
plot(model1, ask = FALSE)
```



```{r, eval = FALSE}
outlier <- car::outlierTest(model1)
outlier <- rbind.data.frame(outlier)
kable_styling(kable(outlier), position='center')
```

We see that one point has a large rstudent residual, but given the number of points in the model this is not a very wild residual value, as indicated by the Bonferonni-Adjusted P value of 0.69, indicating that this is not a significantly distant outlier.

  
We also checked the largest Cookâ€™s distance, which is much less than one, indicating that there are no points with very large leverage in this model. From the plot wee that the points with large cook's distance do not appear to be significantly distant from the other points in the dataset, and their influence is not undue. 

## Sumnmary and conclusions

```{r, eval = FALSE}
final_model <- model1
options(scipen=999)

tb_final <- broom::tidy(final_model, conf.level=0.95,
                   conf.int = TRUE, exponentiate=F)

kable(tb_final, digits = 3,
      caption="Coefficient Summary for Final Model") %>%
  kable_styling(latex_options = "hold_position")

# Coefs and C.I.
df.lm <- data.frame(var = names(model1$coefficients),
                    coef = model1$coefficients,
                    lwr = confint(model1)[,1],
                    upr = confint(model1)[,2],
                    row.names = NULL)
# plot of c.i.

ggplot() +
  geom_errorbar(data = df.lm, aes(x = var, ymin = lwr, ymax = upr), size = 0.3) +
  geom_hline(aes(yintercept = 0), color = "red") +
  coord_flip() +
  labs(subtitle = "95 % confidence intervals for the final model")
```

```{r predict-model1, echo=FALSE, eval = FALSE}
predictions = as.data.frame(
  exp(predict(g_interact, newdata=paint_test, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```


### 4. Summary and Conclusions:



---
title: "Part-I Simple Model"
author: "Chenxi Wu, George Lindner, Qianyin Lu, Yi Mi"
date: "11/30/2019"
output: 
    pdf_document:
        highlight: pygment
        toc: false
        toc_depth: 2
        fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, 
                      warning=FALSE, 
                      message=FALSE,
                      fig.align="center",
                      fig.pos='H', 
                      results="asis")
```

```{r pakages}
library(tidyverse)
library(GGally)
library(car)
library(dplyr)
library(kableExtra)
```

```{r read-data}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```

## Introduction

## Data cleaning 

```{r data-manipulation}
# merge the two dataset first
paint <- rbind(paintings_train, paintings_test)
# Fix position > 1
paint$position[paint$position > 1] <- paint$position[paint$position > 1]/100
# regroup some categorical variables
paint <- paint %>%
  mutate(winningbiddertype = as.factor(dplyr::recode(na_if(winningbiddertype,""),
                                              .missing = 'U')),
         endbuyer = as.factor(dplyr::recode(na_if(endbuyer,""),
                                     .missing = 'U')),
         Interm = as.factor(dplyr::recode(Interm,
                                   .missing = 0,
                                  `0` = 0,
                                 `1` = 1)), 
         authorstyle = as.factor(dplyr::recode(authorstyle,
                                               'n/a' = 0,
                                               .default = 1)),
         type_intermed = as.factor(dplyr::recode(na_if(type_intermed,''),
                                          .missing='Unknown')),
         Shape = as.factor(dplyr::recode(na_if(Shape,""),
                                  .missing = 'Unknown',
                                  'squ_rect' = 'squ_rect',
                                  .default = 'other')),
         materialCat = as.factor(dplyr::recode(na_if(materialCat,""),
                                        .missing = 'Unknown')),
         nfigures = as.factor(dplyr::recode(nfigures,
                                            `0`= 0,
                                            .default = 1))
         ) %>% 
  # change variables into appropriate format
  mutate_at(vars(dealer, origin_author, origin_cat, school_pntg, 
                 diff_origin, artistliving, authorstandard, authorstyle, 
                 winningbiddertype, endbuyer, Interm, type_intermed, 
                 material, mat, materialCat, Shape, engraved, original, 
                 prevcoll, othartist, paired, figures, finished, lrgfont, 
                 relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, 
                 arch, mytho, peasant, othgenre, singlefig, portrait, 
                 still_life, discauth, history, allegory, pastorale,other), as.factor) %>% 
  mutate_at(vars(price), as.numeric) 
```

```{r}
#determine which variable to choose, figures, nfigures and singlefig
paint_train_pre <- paint%>%filter(!is.na(logprice))
eda_test <- cbind(logprice = paint_train_pre$logprice, paint_train_pre[,c(33,39,52)]) %>%
  gather(key = "cat", value = "value", -logprice)
ggplot(eda_test, aes(x = value, y = logprice, colour = cat)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~cat, scales = "free") +
  labs(x = "", title = "logprice vs nfigures,figures and singlefig") +
  theme_bw()
```
```{r}
tab_ori <- table(paint_train_pre$origin_author, paint_train_pre$origin_cat)
chi <- chisq.test(tab_ori)
chi
```


```{r}
# delete unnecessary variables
paint_preselect <- paint %>% 
  dplyr::select(-c("sale", "lot","school_pntg","origin_author","diff_origin", "count","subject", "author", "authorstandard","winningbidder","winningbiddertype","other","pastorale","allegory","history", "lands_ment","original","portrait", "still_life","type_intermed","material","mat","figures","singlefig","Diam_in","Surface_Rnd","Surface_Rect","Width_in","Height_in","price"))
#notice some NAs and 0s in Surface variable, needs to impute them with median
paint_preselect <- paint_preselect%>% mutate(Surface = dplyr::recode(Surface,.missing = median(paint$Surface, na.rm = T)))
paint_preselect$Surface[paint_preselect$Surface==0] <- median(paint$Surface, na.rm = T)
summary(paint_preselect)
paint_train <- paint_preselect%>%filter(!is.na(logprice))
paint_test <-paint_preselect%>%filter(is.na(logprice))
```



## EDA
Using EDA and any numerical summaries get to know the data -  identify what you might consider the 10 best variables for predicting `logprice` using scatterplots with other variables represented using colors or symbols, scatterplot matrices or conditioning plots. 

```{r eda}
hist1 <- ggplot(paint_train, aes(x = logprice)) +
  geom_histogram(fill = "light blue") +
  labs(title = "Empirical distribution for `logprice`")
hist2 <- ggplot(paint_train, aes(x = price)) +
  geom_histogram(fill = "light blue") +
  labs(title = "Empirical distribution for `price`")
# create dataframe for eda plots
eda_cat <- cbind(logprice = paint_train$logprice, 
                 paint_train[, map_chr(paint_train, class) == "factor"])
eda_con <- paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>% 
  dplyr::select(c(3, 1:4))
eda_cat <- eda_cat %>% 
  gather(key = "categorical", value = "value", -logprice)
eda_con <- eda_con %>% 
  gather(key = "numeric", value  ="value", -logprice)
# split eda_cat since there are too many variables
eda_cat1 <- eda_cat[c(1:19500), ]
eda_cat2 <- eda_cat[c(19501:37500), ]
ggplot(eda_cat1, aes(x = value, y = logprice, colour = categorical)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~categorical, scales = "free") +
  labs(x = "", title = "logprice vs categorical predictors") +
  theme_bw()
ggplot(eda_cat2, aes(x = value, y = logprice, colour = categorical)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~categorical, scales = "free") +
  labs(x = "", title = "logprice vs categorical predictors") +
  theme_bw()
ggplot(eda_con, aes(x = value, y = logprice)) +
  geom_point(size = 0.5, alpha = 0.5) +
  stat_smooth(method = "lm") +
  facet_wrap(~numeric, scales = "free") +
  labs(x = "", title = "logprice vs continuous predictors") +
  theme_bw()
ggpairs(paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>% 
  dplyr::select(c(3,1:4)), lower = list(continuous = wrap("points", alpha = 0.5, size = 0.5))) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Pairwise Comparisons of Continuous Variables")
```

transformation graph:
```{r transformation}
paint.train_plt_trans <- paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>%
  filter(Surface!=0)%>%
  mutate(Surface = log(Surface)) %>%
  tidyr::gather(key = "variables", value = "values", -logprice)
ggplot(paint.train_plt_trans, aes(x = values, y = logprice))+
geom_point(alpha = 0.5, size = 0.7)+
facet_wrap(~variables, scales = "free_x")+
labs(x = "value", y = "logprice",
title = "logprice vs Transformed Quantitative Predictors") +
theme_linedraw()+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


## Build your first model

In the first model predict the auction price `price` using the transformation `logprice` using at least 10 and up to 20 predictors and any interactions to build a model using linear regression. 

# variables selection and model selection

```{r}
# data preselection
paint_train_trans <- paint_train %>% 
  dplyr::select(-c(price, othartist, mytho, relig, landsALL)) %>%
  dplyr::mutate(Surface = log(Surface))

# use bic to select the variables which could be used to build interaction terms
model.0 <- lm(logprice~., data = paint_train_trans)
summary(model.0)

model.0.bic = step(model.0, k=log(nrow(paint_train_trans)), trace = F)
summary(model.0.bic)

# use the result fomula of bic to find interactions with aic
model.1 = lm(logprice ~ (dealer + year + origin_cat + artistliving + 
                           authorstyle + winningbiddertype + Surface + engraved + prevcoll + 
                           paired + finished + lrgfont + lands_sc + portrait + still_life + 
                           discauth)^2, data = paint_train_trans)
model.1.aic = step(model.1, k = 2, trace = F)
summary(model.1.aic)

```

We first remove some variables because theyr have very similar levels and thus may not have significant influence on the response.

We use BIC to select the important variables. And we use the BIC result to build a new model and construct all interaction terms and then we use AIC to determine the final model.(may not appear in the write-up for bic part)

We deleted some interactions for the follow reasons. We deleted the interactions `lands_sc:discauth`, `paired:lands_sc` and `artistliving:lands_sc`. First of all, for the interactions related to `lands_sc`, we think the `lands_sc`--if described as a plain landscape, is a variable describing a single dimension of painting content and we don't think it will interact with the variables describing the dealers' behaviour, the pairing of a painting, or the living info of the artists. Though the Adjusted R-squared decreased a little, from 0.66 to 0.659, we think it worth to make the model more reasonable.

# finalmodel 

```{r}
model1 <- lm(formula = logprice ~ dealer + year + origin_cat + artistliving + 
                   authorstyle + endbuyer + Interm + Surface + engraved + prevcoll + 
                   paired + finished + lrgfont + lands_sc + discauth + dealer:year + 
                   + dealer:Interm + dealer:discauth + year:artistliving + 
                   year:endbuyer + year:Surface+ year:discauth + 
                   artistliving:authorstyle + artistliving:discauth + 
                   authorstyle:Surface + endbuyer:finished + 
                   endbuyer:discauth + Interm:Surface + Interm:prevcoll + 
                   Interm:lrgfont + Interm:discauth + Surface:discauth + 
                   prevcoll:finished + paired:finished + 
                   paired:lrgfont + paired:discauth, data = paint_train_trans)
summary(model1)
```


```{r}
# plot
par(mfrow = c(2, 2))
plot(model1, ask = FALSE)
```

The plot identified leverage points 218 and 1129 which should be removed. We removed these two points and refit the model.

# finalmodel refit

```{r}
paint_train_trans = paint_train_trans[-c(218,1129),]
model1 <- lm(formula = logprice ~ dealer + year + origin_cat + artistliving + 
                   authorstyle + endbuyer + Interm + Surface + engraved + prevcoll + 
                   paired + finished + lrgfont + lands_sc + discauth + dealer:year + 
                   + dealer:Interm + dealer:discauth + year:artistliving + 
                   year:endbuyer + year:Surface+ year:discauth + 
                   artistliving:authorstyle + artistliving:discauth + 
                   authorstyle:Surface + endbuyer:finished + 
                   endbuyer:discauth + Interm:Surface + Interm:prevcoll + 
                   Interm:lrgfont + Interm:discauth + Surface:discauth + 
                   prevcoll:finished + paired:finished + 
                   paired:lrgfont + paired:discauth, data = paint_train_trans)
summary(model1)
```

```{r, warning=F}
# plot
par(mfrow = c(2, 2))
plot(model1, ask = FALSE)
# Coefs and C.I.
df.lm <- data.frame(var = names(model1$coefficients),
                    coef = model1$coefficients,
                    lwr = confint(model1)[,1],
                    upr = confint(model1)[,2],
                    row.names = NULL)
kable(df.lm, digits = 3, caption = "Coefs and C.I. for the final model")
# plot of c.i.
ggplot() +
  geom_errorbar(data = df.lm, aes(x = var, ymin = lwr, ymax = upr), size = 0.3) +
  geom_hline(aes(yintercept = 0), color = "red") +
  coord_flip() +
  labs(subtitle = "95 % confidence intervals for the final model")
```

```{r}
car::outlierTest(model1)
```

We see that one point has a large rstudent residual, but given the number of points in the model this is not a very wild residual value, as indicated by the Bonferonni-Adjusted P value of 0.69, indicating that this is not a significantly distant outlier.

```{r}
max(cooks.distance(model1))
```

We see that the largest Cook’s distance is much less than one, indicating that there are no points with very large leverage in this model. From the plot wee that the points with large cook's distance do not appear to be significantly distant from the other points in the dataset, and their influence is not undue. 



Save predictions and intervals.  
```{r test-data-manipulation}
# change variables into appropriate format
paint_test <- paint_test %>% 
  mutate_at(vars(dealer, origin_author, origin_cat, school_pntg, diff_origin, artistliving, authorstandard, authorstyle, winningbiddertype, endbuyer, Interm, type_intermed, material, mat, materialCat, Shape, engraved, original, prevcoll, othartist, paired, figures, finished, lrgfont, relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, arch, mytho, peasant, othgenre, singlefig, portrait, still_life, discauth, history, allegory, pastorale), as.factor) %>% 
  mutate_at(vars(price), as.numeric)

# delete unnecessary variables
paint_test <- paint_test %>% 
  dplyr::select(-c("sale", "lot", "count", "subject", "author", "authorstandard"))
summary(paint_test)
```

```{r predict-model1, echo=FALSE}
predictions = as.data.frame(
  exp(predict(model1, newdata=paint_test, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```



Once you are satisfied with your model, provide a write up of your data analysis project in a new Rmd file/pdf file: `Part-I-Writeup.Rmd` by copying over salient parts of your R notebook. The written assignment consists of five parts:

1. Introduction: Summary of problem and objectives (5 points)

2. Exploratory data analysis (10 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.

3. Development and assessment of an initial model (10 points)

* Initial model: must include a summary table and an explanation/discussion for variable selection and overall amount of variation explained. 

* Model selection: must include a discussion

* Residual: must include residual plot(s) and a discussion.  

* Variables: must include table of coefficients and CI

4. Summary and Conclusions (10 points)

What is the (median) price for the "baseline" category if there are categorical or dummy variables in the model (add CI's)?  (be sure to include units!) Highlight important findings and potential limitations of your model.  Does it appear that interactions are important?  What are the most important variables and/or interactions?  Provide interprations of how the most important variables influence the (median) price giving a range (CI).  Correct interpretation of coefficients for the log model desirable for full points.

Provide recommendations for the art historian about features or combination of features to look for to find the most valuable paintings.

_Points will be deducted for code chunks that should not be included, etc._

*Upload write up  to Sakai any time before Dec 7th*

###  Evaluation on test data for Part I

Once your write up is submitted, your models will be evaluated on the following criteria based on predictions  on the test data (20 points): 

* Bias:  Average (Yhat-Y)  positive values indicate the model tends to overestimate price (on average) while negative values indicate the model tends to underestimate price.

* Maximum Deviation:  Max |Y-Yhat| -  identifies the worst prediction  made in the validation data set.

* Mean Absolute Deviation:  Average |Y-Yhat| - the average error (regardless of sign).

* Root Mean Square Error: Sqrt Average (Y-Yhat)^2

* Coverage:  Average( lwr < Y < upr) 

In order to have a passing wercker badge, your file for predictions needs to be the same length as the test data, with three columns:  fitted values, lower CI and upper CI values in that order with names, *fit*, *lwr*, and *upr* respectively such as in the code chunk below. 

Save predictions and intervals.  
```{r predict-model-final, echo=FALSE, include=FALSE}
# change model1 or update as needed
predictions = as.data.frame(
  exp(predict(model1, newdata=paintings_test, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```


You will be able to see your scores on the score board.  They will be initialized by a prediction based on the mean in the training data.






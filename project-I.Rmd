---
title: "Part-I Simple Model"
author: "Chenxi Wu, George Lindner, Qianyin Lu, Yi Mi"
date: "11/30/2019"
output: 
    pdf_document:
        highlight: pygment
        toc: false
        toc_depth: 2
        fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, 
                      warning=FALSE, 
                      message=FALSE,
                      fig.align="center",
                      fig.pos='H', 
                      results="asis")
```

```{r pakages}
library(tidyverse)
library(GGally)
library(car)
library(dplyr)
library(kableExtra)
```

```{r read-data}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
```

## Introduction
need intro
  

## Data Manipulation and EDA 

```{r data-manipulation}
# merge the two dataset first
paint <- rbind(paintings_train, paintings_test)
# Fix position > 1
paint$position[paintings_train$position > 1] <- paint$position[paint$position > 1]/100

# Fix material type
paint$material[str_detect(paint$material, "^bois")] <- "bois"
paint$material[str_detect(paint$material, "^cuivre")] <- "cuivre"
paint$material[str_detect(paint$material, "^tableau")] <- "tableau"
paint$material[str_detect(paint$material, "^toile")] <- "toile"

# regroup some categorical variables
paint <- paint %>%
  mutate(winningbiddertype = as.factor(dplyr::recode(na_if(winningbiddertype,""),
                                                     .missing = 'U')),
         endbuyer = as.factor(dplyr::recode(na_if(endbuyer,""),
                                            .missing = 'U')),
         Interm = as.factor(dplyr::recode(Interm,
                                          .missing = 0,
                                          `0` = 0,
                                          `1` = 1)), 
         authorstyle = as.factor(dplyr::recode(authorstyle,
                                               'n/a' = 0,
                                               .default = 1)),
         type_intermed = as.factor(dplyr::recode(na_if(type_intermed,''),
                                                 .missing='Unknown')),
         Shape = as.factor(dplyr::recode(na_if(Shape,""),
                                         .missing = 'Unknown',
                                         'squ_rect' = 'squ_rect',
                                         .default = 'other')),
         materialCat = as.factor(dplyr::recode(na_if(materialCat,""),
                                               .missing = 'Unknown')),
         material = as.factor(dplyr::recode(na_if(material,""),
                                     .missing = 'unknown',
                                     'n/a' = 'unknown', 
                                     'bois' = 'bois', 
                                     'cuivre' = 'cuivre', 
                                     'tableau' = 'tableau', 
                                     'toile' = 'toile', 
                                     .default = 'other')), 
         mat = as.factor(dplyr::recode(na_if(mat,""),
                                     .missing = 'unknown',
                                     'n/a' = 'unknown', 
                                     'b'='wood', 
                                     'c'='copper',
                                     'o'='other', 
                                     't'='canvas', 
                                     'ta'='canvas',  
                                     .default = 'other')),
         nfigures = as.factor(dplyr::recode(nfigures,
                                            `0`= 0,
                                            .default = 1))
         ) %>% 
  # change variables into appropriate format
  mutate_at(vars(dealer, origin_author, origin_cat, school_pntg, 
                 diff_origin, artistliving, authorstandard, authorstyle, 
                 winningbiddertype, endbuyer, Interm, type_intermed, 
                 material, mat, materialCat, Shape, engraved, original, 
                 prevcoll, othartist, paired, figures, finished, lrgfont, 
                 relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, 
                 arch, mytho, peasant, othgenre, singlefig, portrait, 
                 still_life, discauth, history, allegory, pastorale), as.factor) %>% 
  mutate_at(vars(price), as.numeric)
# delete unnecessary variables
paint <- paint %>%
  dplyr::select(-c(sale, lot, count, subject, author, authorstandard, winningbidder, other, Diam_in, Surface_Rnd, history, original, type_intermed, pastorale))
#notice some NAs and 0s in Surface variable, needs to impute them with median
paint<- paint%>% mutate(Surface = dplyr::recode(Surface,.missing = median(paint$Surface, na.rm = T)))
paint$Surface[paint$Surface==0] <- median(paint$Surface, na.rm = T)
summary(paint)
```



```{r}
# delete unnecessary variables
# paint_preselect <- paint %>% 
#   dplyr::select(-c("sale", "lot","school_pntg","origin_author","diff_origin", "count","subject", "author", "authorstandard","winningbidder","winningbiddertype","other","pastorale","allegory","history", "lands_ment","original","portrait", "still_life","type_intermed","material","mat","Diam_in","Surface_Rnd","Surface_Rect","Width_in","Height_in","price"))

# summary(paint_train)
paint_train <- paint%>%
  filter(!is.na(logprice))
paint_test <-paint%>%
  filter(is.na(logprice))
```

Looking at the summary of the dataset (See Appendix), we made following manipulation to the variables: 
 
 - *history*, *original*, *type_intermed*, and *pastorale* have serious **class imbalance** problem, where certain value is seldomly observed. So we will discard these variables in modeling.    
 - The variable *authorstyle* contains too many NA's which is not sufficient to do any analysis, which will also be discarded. 
(need to add description to each variable deleted above, but it actually makes more sense to delete just a few of them before eda, then delete the rest looking at the eda)  
  
To start with, we first check on the emprirical distribution of the response variable. There are 2 variables, *logprice* and *price*. From the histogram (See Appendix), we can see that *logprice*, which is the logarithm of *price*, is more normally-distributed. Consider the normality assumption of linear regression, we will use *logprice* as the response variable.  

```{r eda}
# create dataframe for eda plots
eda_cat <- cbind(logprice = paint_train$logprice, 
                 paint_train[, map_chr(paint_train, class) == "factor"])
eda_con <- paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>% 
  dplyr::select(c(3, 1:2, 5:8))
eda_cat <- eda_cat %>% 
  gather(key = "categorical", value = "value", -logprice)
eda_conn <- eda_con %>% 
  gather(key = "numeric", value  ="value", -logprice)

# split eda_cat since there are too many variables
eda_cat1 <- eda_cat[c(1:27750), ]
eda_cat2 <- eda_cat[c(27751:55500), ]
ggplot(eda_cat1, aes(x = value, y = logprice, colour = categorical)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~categorical, scales = "free") +
  labs(x = "", title = "logprice vs categorical predictors") +
  theme_bw()
ggplot(eda_cat2, aes(x = value, y = logprice, colour = categorical)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~categorical, scales = "free") +
  labs(x = "", title = "logprice vs categorical predictors") +
  theme_bw()
ggplot(eda_conn, aes(x = value, y = logprice)) +
  geom_point(size = 0.5, alpha = 0.5) +
  stat_smooth(method = "lm") +
  facet_wrap(~numeric, scales = "free") +
  labs(x = "", title = "logprice vs continuous predictors") +
  theme_bw()

ggpairs(eda_con, lower = list(continuous = wrap("points", alpha = 0.5, size = 0.5))) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Pairwise Comparisons of Continuous Variables")
```

Then we plot *logprice* against all continuous and categorical variables respectively. Here are some interesting findings.     
  
For categorical variables: 
 
 - After cleaning the data, *mat*, *material* and *materialCat* record duplicate features. The Chi-squared test shows a strong correlation between these variables (See Appendix). We will only keep *matCat* in future analysis since it contains all the information in the other two. 
 
 - *origin_author* and *origin_cat* record similar features which is suspecious of highly dependent on each other. We implement Chi-squared test (See Appendix) to test the hypothesis, which yield a p-value of $2.2e-16$. Thus we will only keep *origin_cat* since it contains less levels.  
 
 - At glance, strong predictors includde *dealer*, *diff_origin*, *discauth*, *endbuyer*, *materialCat*, *interm*, *finished*, *engraved*, *figures*, *Irgfont*, *origin_cat*, *paired*, *portrait*, *prevcoll*, *lands_sc*, *lands_elem*. (Might need adjustments) 

  
For continuous variables: 
 
 - *other* and *position* have most of their values gather around 0 and we cannot observe any obvious pattern.  
 - *Diam-in* and *Surface_Rnd* contain too many NA's and therefore should not be inlcuded in the model.  
 - *Surface*, *Surface_Rnd* and *Surface_Rect* contain duplicate information, and *Surface* contains all the information in *Surface_Rect*. So we will only keep *Surface*. *Height_in* and *Width_in* are discarded for similar reasoning.  
 - *nfigures* might need transformations, so I will exponentiate it in the model. 
 - Additionally, *figures*, *nfigures* and *siglefig* depict duplicate information. Looking at the empirical distribution, we decide to use
 


## Model Selection




```{r}
# data preselection
paint_train_trans <- paint_train %>% 
  dplyr::select(-c(othartist, mytho, relig, landsALL)) %>%
  dplyr::mutate(Surface = log(Surface))

# use bic to select the variables which could be used to build interaction terms
model.0 <- lm(logprice ~ position + dealer + year + origin_cat + artistliving + 
    authorstyle + endbuyer + Interm + Shape + Surface + materialCat + 
    nfigures + engraved + prevcoll + paired + finished + lrgfont + 
    lands_sc + lands_elem + lands_figs + arch + peasant + othgenre + 
    discauth, data = paint_train_trans)
# summary(model.0)

model.0.bic = step(model.0, k=log(nrow(paint_train_trans)), trace = F)
# summary(model.0.bic)

# use the resulting fomula of bic to find interactions with aic
model.1 = lm(formula = logprice ~ (dealer + year + origin_cat + artistliving + 
    authorstyle + endbuyer + Interm + Surface + engraved + prevcoll + 
    paired + finished + lrgfont + lands_sc + discauth)^2, data = paint_train_trans)
# model.1.aic = step(model.1, k = 2, trace = F)
# summary(model.1.aic)

```

We first remove some variables because they have very similar levels and thus may not have significant influence on the response.

We use BIC to select the important variables. And we use the BIC result to build a new model and construct all interaction terms and then we use AIC to determine the final model.(may not appear in the write-up for bic part)

We deleted some interactions for the follow reasons. We deleted the interactions `lands_sc:discauth`, `paired:lands_sc` and `artistliving:lands_sc`. First of all, for the interactions related to `lands_sc`, we think the `lands_sc`--if described as a plain landscape, is a variable describing a single dimension of painting content and we don't think it will interact with the variables describing the dealers' behaviour, the pairing of a painting, or the living info of the artists. Though the Adjusted R-squared decreased a little, from 0.66 to 0.659, we think it worth to make the model more reasonable.



```{r}
model1 <- lm(formula = logprice ~ dealer + year + origin_cat + artistliving + 
                   authorstyle + endbuyer + Interm + Surface + engraved + prevcoll + 
                   paired + finished + lrgfont + lands_sc + discauth + dealer:year + 
                   + dealer:Interm + dealer:discauth + year:artistliving + 
                   year:endbuyer + year:Surface+ year:discauth + 
                   artistliving:authorstyle + artistliving:discauth + 
                   authorstyle:Surface + endbuyer:finished + 
                   endbuyer:discauth + Interm:Surface + Interm:prevcoll + 
                   Interm:lrgfont + Interm:discauth + Surface:discauth + 
                   prevcoll:finished + paired:finished + 
                   paired:lrgfont + paired:discauth, data = paint_train_trans)
```

## Model Assessment

```{r res plot, fig.asp=1}
# plot
par(mfrow = c(2, 2))
plot(model1, ask = FALSE)
```
looking at the diagnostic plots, our model 1 seems to satisfy the assumptions of linear regression resonablly well. From the Residual vs Fitted plot we can see equally spread residuals around a horizontal line without any distinct patterns; The Normal Q-Q plot shows the residuals are almost normally-distributed. The Scale-Location plot shows that homoscedasticity is met. The Residual vs Leverage plot shows that most of the points are not influential. There are only very few points that falls outside of Cook's distance line. The plot identified the influential observation as #218 and #1129. We removed these two points and refitted the model. But removing the two points does not improve the R^2 nor does it help with the residual diagnostics plots (See Appendix the residual plot after refitting) So we will set Model 1 to our final model in part-I. 


```{r}
# delete outliers and refit
paint_train_trans = paint_train_trans[-c(218,1129),]
model1.1 <- lm(formula = logprice ~ dealer + year + origin_cat + artistliving + 
                   authorstyle + endbuyer + Interm + Surface + engraved + prevcoll + 
                   paired + finished + lrgfont + lands_sc + discauth + dealer:year + 
                   + dealer:Interm + dealer:discauth + year:artistliving + 
                   year:endbuyer + year:Surface+ year:discauth + 
                   artistliving:authorstyle + artistliving:discauth + 
                   authorstyle:Surface + endbuyer:finished + 
                   endbuyer:discauth + Interm:Surface + Interm:prevcoll + 
                   Interm:lrgfont + Interm:discauth + Surface:discauth + 
                   prevcoll:finished + paired:finished + 
                   paired:lrgfont + paired:discauth, data = paint_train)
# summary(model1.1)
```


```{r outlier}
outlier <- car::outlierTest(model1)
outlier <- rbind.data.frame(outlier)
kable_styling(kable(outlier), position='center')
```

We see that one point has a large rstudent residual, but given the number of points in the model this is not a very wild residual value, as indicated by the Bonferonni-Adjusted P value of 0.69, indicating that this is not a significantly distant outlier.
  

We also checked the largest Cookâ€™s distance, which is much less than one, indicating that there are no points with very large leverage in this model. From the plot wee that the points with large cook's distance do not appear to be significantly distant from the other points in the dataset, and their influence is not undue. 

## Sumnmary and conclusions
```{r coeff plot}
final_model <- model1
options(scipen=999)
tb_final <- broom::tidy(final_model, conf.level=0.95,
                   conf.int = TRUE, exponentiate=F)
kable_styling(kable(tb_final, digits = 3,
                    caption="Coefficient Summary for Final Model"), 
      position = 'center')
```

```{r ci plot}
# Coefs and C.I.
df.lm <- data.frame(var = names(model1$coefficients),
                    coef = model1$coefficients,
                    lwr = confint(model1)[,1],
                    upr = confint(model1)[,2],
                    row.names = NULL)

# plot of c.i.
ggplot() +
  geom_errorbar(data = df.lm, aes(x = var, ymin = lwr, ymax = upr), size = 0.3) +
  geom_hline(aes(yintercept = 0), color = "red") +
  coord_flip() +
  labs(subtitle = "95 % confidence intervals for the final model")
```


```{r test-set-trans}
paint_test_trans <- paint_test %>% 
  dplyr::mutate(Surface = log(Surface))
```



```{r predict-model1, echo=FALSE}
predictions = as.data.frame(
  exp(predict(model1, newdata=paint_test_trans, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```


## Appendix

```{r histogram}
hist1 <- ggplot(paint_train, aes(x = logprice)) +
  geom_histogram(fill = "light blue") +
  labs(title = "Empirical distribution for `logprice`")
hist2 <- ggplot(paint_train, aes(x = price)) +
  geom_histogram(fill = "light blue") +
  labs(title = "Empirical distribution for `price`")
```


```{r figures}
# determine which variable to choose, figures, nfigures and singlefig
paint_train_pre <- paint %>%
  filter(!is.na(logprice))
eda_test <- cbind(logprice = paint_train_pre$logprice, paint_train_pre[,c("nfigures", "figures", "singlefig")]) %>%
  gather(key = "cat", value = "value", -logprice)
tab_fig <- table(paint_train_pre$figures, paint_train_pre$singlefig)
ggplot(eda_test, aes(x = value, y = logprice, colour = cat)) +
  geom_boxplot(show.legend = F) +
  facet_wrap(~cat, scales = "free") +
  labs(x = "", title = "logprice vs nfigures,figures and singlefig") +
  theme_bw()
```


```{r chi}
tab_ori <- table(paint_train$origin_author, paint_train$origin_cat)
kable(tab_ori)
chi1 <- chisq.test(tab_ori)
tab_material <- table(paint_train$material, paint_train$materialCat)
chi2 <- chisq.test(tab_material)
tab_mat <- table(paint_train$mat, paint_train$materialCat)
chi3 <- chisq.test(tab_mat)
```

```{r transformation}
paint.train_plt_trans <- paint_train[, map_chr(paint_train, class) == "numeric" |
                         map_chr(paint_train, class) == "integer"] %>%
  filter(Surface!=0)%>%
  mutate(Surface = log(Surface)) %>%
  tidyr::gather(key = "variables", value = "values", -logprice)
ggplot(paint.train_plt_trans, aes(x = values, y = logprice))+
geom_point(alpha = 0.5, size = 0.7)+
facet_wrap(~variables, scales = "free_x")+
labs(x = "value", y = "logprice",
title = "logprice vs Transformed Quantitative Predictors") +
theme_linedraw()+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



```{r new-res-plot}
# summary
summary(model1)
# plot
par(mfrow = c(2, 2))
plot(model1.1, ask = FALSE)
```

```{r cook}
lcook <- max(cooks.distance(model1))
```
